library(pROC)
roc_obj <- roc(test_data$severity, as.numeric(y_pred))
auc_roc <- auc(roc_obj)
print(paste("AUC-ROC:", auc_roc))
}
# Calculate training accuracy
train_predictions <- predict(svm_model, newdata = train_data)
train_accuracy <- sum(train_predictions == train_data$severity) / nrow(train_data)
print(paste("Training Accuracy:", train_accuracy))
# Calculate testing accuracy
test_accuracy <- sum(y_pred == test_data$severity) / nrow(test_data)
print(paste("Testing Accuracy:", test_accuracy))
# Calculate True Negative Rate (TNR)
TN <- confusion[1, 1]  # True negatives
FP <- confusion[1, 2]  # False positives
TNR <- TN / (TN + FP)
print(paste("TNR:", TNR))
#---- SVM-Radial
library(e1071)
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(data4), 0.7 * nrow(data4))  # 70% for training
train_data <- data4[train_indices, ]
test_data <- data4[-train_indices, ]
# Train the SVM model
svm_model <- svm(formula = severity ~ ., kernel = "radial", data = train_data)
# Make predictions on the test data
y_pred <- predict(svm_model, newdata = test_data)
y_pred <- ifelse(y_pred >= 0.5, "1", "0")
# Calculate accuracy
accuracy <- sum(y_pred == test_data$severity) / nrow(test_data)
print(paste("Accuracy:", accuracy))
# Calculate confusion matrix
confusion <- table(Actual = test_data$severity, Predicted = y_pred)
print("Confusion Matrix:")
print(confusion)
# Calculate precision, recall, and F1-score
#precision <- confusion[2, 2] / sum(confusion[, 2])
#recall <- confusion[2, 2] / sum(confusion[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-Score:", f1_score))
# Calculate AUC-ROC (if applicable)
if (length(levels(test_data$severity)) == 2) {
library(pROC)
roc_obj <- roc(test_data$severity, as.numeric(y_pred))
auc_roc <- auc(roc_obj)
print(paste("AUC-ROC:", auc_roc))
}
# Calculate training accuracy
train_predictions <- predict(svm_model, newdata = train_data)
train_accuracy <- sum(train_predictions == train_data$severity) / nrow(train_data)
print(paste("Training Accuracy:", train_accuracy))
# Calculate testing accuracy
test_accuracy <- sum(y_pred == test_data$severity) / nrow(test_data)
print(paste("Testing Accuracy:", test_accuracy))
# Calculate True Negative Rate (TNR)
TN <- confusion[1, 1]  # True negatives
FP <- confusion[1, 2]  # False positives
TNR <- TN / (TN + FP)
print(paste("TNR:", TNR))
svm_model
# script to read in raw data and organize it
library(dplyr)
library(readxl)
library(tidyr)
library(stringr)
library(stringi)
library(lubridate)
library(ggmap)
library(ggplot2)
library(caTools)
#setwd('/Users/emmaboudreau/Documents/GitHub/697proj/')
setwd('/Users/samuelesquivel/Documents/GitHub/697project/')
###PREP/MUNGE###
# read in the data-----
data = read.csv('export.csv')
#---- 1 dataframe we could use
data2 = select(data,-c(dist_dirc_exit, age, max_injr_svrty_cl,
numb_fatal_injr, numb_nonfatal_injr,injy_stat_descr,
vehc_unit_numb,crash_status,max_injr_svrty_vl, pers_numb))%>%
filter(!is.na(speed_limit))%>% #filter out anything without speed limit
rename("severity" = "crash_severity_descr")%>% #changed column name
rename("weather" = "weath_cond_descr")%>%
mutate(severity=ifelse(severity=="Property damage only (none injured)",0,
ifelse(severity=="Non-fatal injury",1,ifelse(severity=="Fatal injury",2,3)
)
)
)%>%
filter(severity!="3")%>% #remove any unreported or unknown severity levels
filter(severity!="2")%>% #remove fatal injuries
drop_na()%>% #drop any row with NA
mutate(weather = ifelse(weather =="Clear/Clear","Clear",ifelse(weather == "Rain/Rain","Rain",
ifelse(weather=="Not Reported","Unknown",ifelse(weather=="Snow/Snow","Snow",
ifelse(weather=="Cloudy/Rain","Rain",ifelse(weather=="Clear/Cloudy", "Cloudy",
ifelse(weather=="Snow/Cloudy","Snow",ifelse(weather=="Clear/Blowing sand, snow","Snow",
ifelse(weather=="Rain/Sleet, hail (freezing rain or drizzle)","Rain", ifelse(weather=="Snow/Blowing sand, snow", "Snow",
ifelse(weather=="Clear/Rain","Rain",ifelse(weather=="Cloudy/Cloudy","Cloudy",
ifelse(weather=="Rain/Cloudy","Rain",ifelse(weather=="Rain/Severe crosswinds","Rain",
ifelse(weather=="Snow/Sleet, hail (freezing rain or drizzle)", "Snow",weather)))))))))))
)))))%>% #concatenating weather conditions
mutate(weather=ifelse(weather=="Clear",0,
ifelse(weather=="Cloudy",1,ifelse(weather=="Snow",2,
ifelse(weather=="Rain",3,
ifelse(weather=="Unknown",5,6)
)
)
)
)
)%>% #creating numerical code for different weather conditions
#filter(weather!="5")
filter(weather!="5")%>% #filtering out any unknown or other weather descriptions that are not frequently used/ambiguous
filter(weather!="6")
WS<-as.Date("12/15/2018", format=  "%m/%d/%Y") #winter solstice
SE<-as.Date("03/15/2018", format=  "%m/%d/%Y") #spring equinox
SS<-as.Date("06/15/2018", format=  "%m/%d/%Y") #summer solstice
FE<-as.Date("09/15/2018", format=  "%m/%d/%Y") #fall equinox
#seasons code: winter = 0, spring = 1, summer = 2, fall = 3
data3 = data2%>%
rename("date"="crash_date")%>%
mutate(date = as.Date(date,format= "%m/%d/%Y"))%>%
mutate(season = ifelse(date>=WS | date<SE, "0", ifelse(date>=SE & date< SS, "1",
ifelse(date>=SS&date< FE, "2", "3"))))
#convert existing date and time to standard POSIX format
data3 <- data3 %>%
mutate(crash_time_2 = str_replace(crash_time_2, "\\s(AM|PM)", " \\1"),  # Remove the space before AM/PM
crash_time_2 = as.POSIXct(crash_time_2, format = "%I:%M %p"),  # Convert to POSIXct format
crash_date = as.Date(date))  # Convert 'date' to Date format
#mutate data3 to standard time
data3 <- data3 %>%
mutate(crash_date_time_standard = as.POSIXct(paste(crash_date, format(crash_time_2, "%H:%M:%S")), format = "%Y-%m-%d %H:%M:%S"))
#drop the excess columns we don't need anymore and make new polished data frame
data4 <- data3 %>%
select(-crash_date, -date, -crash_time_2,-crash_numb,-city_town_name)
#drop_na2()%>% #drop any row with NA
###BASIC ANALYSIS###
###Crash count vs time of day###
#do some analysis on the timing of events
#extract the hour from the crash_date_time_standard column
data4 <- data4 %>%
mutate(hour = hour(crash_date_time_standard))
#calculate the crash count for each hour
crash_count <- data4 %>%
count(hour)
#plot the crash count by hour
plot(crash_count$hour, crash_count$n, type = "l", xlab = "Hour of Day", ylab = "Crash Count", main = "Crash Count by Hour")
###BASIC ANALYSIS###
###Crash count vs time of day###
# Do some analysis on the timing of events
# Extract the hour from the crash_date_time_standard column
data4 <- data4 %>%
mutate(hour = hour(crash_date_time_standard))
# Calculate the crash count for each hour
crash_count <- data4 %>%
count(hour)
# Create a pretty plot with colors and additional features
library(ggplot2)
library(scales)
ggplot(crash_count, aes(x = hour, y = n)) +
geom_line(color = "steelblue", size = 1) +
labs(x = "Hour of Day", y = "Crash Count", title = "Crash Count by Hour") +
theme_minimal() +
theme(plot.title = element_text(size = 16, face = "bold"),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
legend.position = "none") +
scale_x_continuous(breaks = seq(0, 23, by = 2)) +
scale_y_continuous(labels = comma) +
annotate("text", x = 23, y = max(crash_count$n), label = "North", hjust = 1, vjust = 1, size = 5, fontface = "bold") +
annotate("text", x = 0, y = max(crash_count$n), label = "South", hjust = 0, vjust = 1, size = 5, fontface = "bold") +
annotate("text", x = 12, y = max(crash_count$n), label = "Scale", hjust = 0.5, vjust = 1, size = 5, fontface = "bold") +
coord_polar(start = -pi/2)
###BASIC ANALYSIS###
###Crash count vs time of day###
#do some analysis on the timing of events
#extract the hour from the crash_date_time_standard column
data4 <- data4 %>%
mutate(hour = hour(crash_date_time_standard))
#calculate the crash count for each hour
crash_count <- data4 %>%
count(hour)
# Create a color palette for the line
line_color <- "steelblue"
# Plot the crash count by hour with colored line
plot(crash_count$hour, crash_count$n, type = "l", xlab = "Hour of Day", ylab = "Crash Count",
main = "Crash Count by Hour", col = line_color)
###BASIC ANALYSIS###
###Crash count vs time of day###
# Do some analysis on the timing of events
# Extract the hour from the crash_date_time_standard column
data4 <- data4 %>%
mutate(hour = hour(crash_date_time_standard))
# Calculate the crash count for each hour
crash_count <- data4 %>%
count(hour)
# Create a bar plot for crash count by hour
barplot(crash_count$n, names.arg = crash_count$hour, xlab = "Hour of Day", ylab = "Crash Count",
main = "Crash Count by Hour", col = "skyblue", border = "black", ylim = c(0, max(crash_count$n) + 10))
###Location in Boston map###
# Load required libraries
library(ggplot2)
library(ggmap)
# Set the latitude and longitude boundaries for Boston area
boston_bounds <- c(left = -71.1912, bottom = 42.2279, right = -70.8085, top = 42.3974)
# Get the map background using ggmap and specify the map type
boston_map <- get_stamenmap(boston_bounds, maptype = "toner-lite")
# Plot the map of Boston with compass, scale, and defined boundaries
ggmap(boston_map) +
# Add points representing crash locations
geom_point(data = data4, aes(x = lon, y = lat), color = "red", alpha = 0.5) +
# Adjust the transparency and color of the points
guides(alpha = FALSE) +
labs(title = "Crashes in Boston, MA") +
theme(plot.title = element_text(hjust = 0.5),
legend.position = "bottom") +
# Add compass
annotation_north_arrow(location = "tl", style = north_arrow_fancy_orienteering) +
# Add scale bar
annotation_scale(location = "br", width_hint = 0.2)
install.packages("ggcompass")
# Load required libraries
library(ggplot2)
library(ggmap)
library(ggcompass)
# Load required libraries
library(ggplot2)
library(ggmap)
# Set the latitude and longitude boundaries for Boston area
boston_bounds <- c(left = -71.1912, bottom = 42.2279, right = -70.8085, top = 42.3974)
# Get the map background using ggmap and specify the map type
boston_map <- get_stamenmap(boston_bounds, maptype = "toner-lite")
# Create a dummy data frame with sample crash locations
crash_data <- data.frame(lon = c(-71.06, -71.09, -71.08), lat = c(42.35, 42.37, 42.38))
# Plot the map of Boston with compass, scale, and more definition
ggmap(boston_map) +
# Add points representing crash locations
geom_point(data = crash_data, aes(x = lon, y = lat), color = "red", alpha = 0.5) +
# Add north arrow
annotate("text", x = -71.19, y = 42.39, label = "\u2191", size = 10, color = "black") +
# Add scale
annotation_scale(location = "tr", width_hint = 0.2) +
# Adjust the transparency and color of the points
guides(alpha = FALSE) +
labs(title = "Crashes in Boston, MA") +
theme(plot.title = element_text(hjust = 0.5))
# Load required libraries
library(ggplot2)
library(ggmap)
# Set the latitude and longitude boundaries for Boston area
boston_bounds <- c(left = -71.1912, bottom = 42.2279, right = -70.8085, top = 42.3974)
# Get the map background using ggmap and specify the map type
boston_map <- get_stamenmap(boston_bounds, maptype = "toner-lite")
# Create a dummy data frame with sample crash locations
crash_data <- data.frame(lon = c(-71.06, -71.09, -71.08), lat = c(42.35, 42.37, 42.38))
# Plot the map of Boston with compass, scale, and more definition
ggmap(boston_map) +
# Add points representing crash locations
geom_point(data = crash_data, aes(x = lon, y = lat), color = "red", alpha = 0.5) +
# Add north arrow
annotate("text", x = -71.19, y = 42.39, label = "\u2191", size = 10, color = "black") +
# Add scale bar
geom_rect(aes(xmin = -71.19, xmax = -71.18, ymin = 42.2279, ymax = 42.2329),
fill = "white", color = "black") +
annotate("text", x = -71.185, y = 42.2289, label = "1 km", color = "black") +
# Adjust the transparency and color of the points
guides(alpha = FALSE) +
labs(title = "Crashes in Boston, MA") +
theme(plot.title = element_text(hjust = 0.5))
###Location in Boston map###
# Load required libraries
library(ggplot2)
library(ggmap)
#set the latitude and longitude boundaries for Boston area
boston_bounds <- c(left = -71.1912, bottom = 42.2279, right = -70.8085, top = 42.3974)
#get the map background using ggmap and specify the map type
boston_map <- get_stamenmap(boston_bounds, maptype = "toner-lite")
#plot the map of Boston
ggmap(boston_map) +
#add points representing crash locations
geom_point(data = data4, aes(x = lon, y = lat), color = "red", alpha = 0.5) +
#adjust the transparency and color of the points
guides(alpha = FALSE) +
labs(title = "Crashes in Boston, MA") +
theme(plot.title = element_text(hjust = 0.5))
###Location in Boston map###
# Load required libraries
library(ggplot2)
library(ggmap)
#set the latitude and longitude boundaries for Boston area
boston_bounds <- c(left = -71.1912, bottom = 42.2279, right = -70.8085, top = 42.3974)
#get the map background using ggmap and specify the map type
boston_map <- get_stamenmap(boston_bounds, maptype = "toner-lite")
#plot the map of Boston
ggmap(boston_map) +
#add points representing crash locations
geom_point(data = data4, aes(x = lon, y = lat), color = "blue", alpha = 0.5) +
#adjust the transparency and color of the points
guides(alpha = FALSE) +
labs(title = "Crashes in Boston, MA") +
theme(plot.title = element_text(hjust = 0.5))
# Load required libraries
library(ggplot2)
library(ggmap)
# Set the latitude and longitude boundaries for the Boston area
boston_bounds <- c(left = -71.1912, bottom = 42.2279, right = -70.8085, top = 42.3974)
# Get the map background using ggmap and specify the map type
boston_map <- get_stamenmap(boston_bounds, maptype = "toner-lite")
# Plot the map of Boston
ggmap(boston_map) +
# Add points representing crash locations
geom_point(data = data4, aes(x = lon, y = lat), color = "blue", alpha = 0.5) +
# Adjust the transparency and color of the points
guides(alpha = FALSE) +
labs(title = "Crashes in Boston, MA") +
theme(plot.title = element_text(hjust = 0.5))
library(e1071)
library(ggplot2)
# Generate a synthetic dataset
set.seed(123)
x1 <- rnorm(100, mean = 2)
y1 <- rnorm(100, mean = 2)
x2 <- rnorm(100, mean = -2)
y2 <- rnorm(100, mean = -2)
data <- data.frame(x = c(x1, x2), y = c(y1, y2), label = factor(rep(c("A", "B"), each = 100)))
# Split the data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Train the SVM model
svm_model <- svm(formula = label ~ ., kernel = "radial", data = train_data)
# Create a grid of points for visualization
x_grid <- seq(min(data$x), max(data$x), length.out = 100)
y_grid <- seq(min(data$y), max(data$y), length.out = 100)
grid <- expand.grid(x = x_grid, y = y_grid)
# Make predictions on the grid points
grid$label <- predict(svm_model, newdata = grid)
# Plot the data points and decision boundary
ggplot() +
geom_point(data = data, aes(x = x, y = y, color = label)) +
geom_contour(data = grid, aes(x = x, y = y, z = as.numeric(label)),
bins = 1, alpha = 0.2, color = "black", linetype = "dashed") +
scale_color_manual(values = c("red", "blue")) +
theme_minimal()
# script to read in raw data and organize it
library(dplyr)
library(readxl)
library(tidyr)
library(stringr)
library(stringi)
library(lubridate)
library(ggmap)
library(ggplot2)
library(caTools)
#setwd('/Users/emmaboudreau/Documents/GitHub/697proj/')
setwd('/Users/samuelesquivel/Documents/GitHub/697project/')
###PREP/MUNGE###
# read in the data-----
data = read.csv('export.csv')
#---- 1 dataframe we could use
data2 = select(data,-c(dist_dirc_exit, age, max_injr_svrty_cl,
numb_fatal_injr, numb_nonfatal_injr,injy_stat_descr,
vehc_unit_numb,crash_status,max_injr_svrty_vl, pers_numb))%>%
filter(!is.na(speed_limit))%>% #filter out anything without speed limit
rename("severity" = "crash_severity_descr")%>% #changed column name
rename("weather" = "weath_cond_descr")%>%
mutate(severity=ifelse(severity=="Property damage only (none injured)",0,
ifelse(severity=="Non-fatal injury",1,ifelse(severity=="Fatal injury",2,3)
)
)
)%>%
filter(severity!="3")%>% #remove any unreported or unknown severity levels
filter(severity!="2")%>% #remove fatal injuries
drop_na()%>% #drop any row with NA
mutate(weather = ifelse(weather =="Clear/Clear","Clear",ifelse(weather == "Rain/Rain","Rain",
ifelse(weather=="Not Reported","Unknown",ifelse(weather=="Snow/Snow","Snow",
ifelse(weather=="Cloudy/Rain","Rain",ifelse(weather=="Clear/Cloudy", "Cloudy",
ifelse(weather=="Snow/Cloudy","Snow",ifelse(weather=="Clear/Blowing sand, snow","Snow",
ifelse(weather=="Rain/Sleet, hail (freezing rain or drizzle)","Rain", ifelse(weather=="Snow/Blowing sand, snow", "Snow",
ifelse(weather=="Clear/Rain","Rain",ifelse(weather=="Cloudy/Cloudy","Cloudy",
ifelse(weather=="Rain/Cloudy","Rain",ifelse(weather=="Rain/Severe crosswinds","Rain",
ifelse(weather=="Snow/Sleet, hail (freezing rain or drizzle)", "Snow",weather)))))))))))
)))))%>% #concatenating weather conditions
mutate(weather=ifelse(weather=="Clear",0,
ifelse(weather=="Cloudy",1,ifelse(weather=="Snow",2,
ifelse(weather=="Rain",3,
ifelse(weather=="Unknown",5,6)
)
)
)
)
)%>% #creating numerical code for different weather conditions
#filter(weather!="5")
filter(weather!="5")%>% #filtering out any unknown or other weather descriptions that are not frequently used/ambiguous
filter(weather!="6")
WS<-as.Date("12/15/2018", format=  "%m/%d/%Y") #winter solstice
SE<-as.Date("03/15/2018", format=  "%m/%d/%Y") #spring equinox
SS<-as.Date("06/15/2018", format=  "%m/%d/%Y") #summer solstice
FE<-as.Date("09/15/2018", format=  "%m/%d/%Y") #fall equinox
#seasons code: winter = 0, spring = 1, summer = 2, fall = 3
data3 = data2%>%
rename("date"="crash_date")%>%
mutate(date = as.Date(date,format= "%m/%d/%Y"))%>%
mutate(season = ifelse(date>=WS | date<SE, "0", ifelse(date>=SE & date< SS, "1",
ifelse(date>=SS&date< FE, "2", "3"))))
#convert existing date and time to standard POSIX format
data3 <- data3 %>%
mutate(crash_time_2 = str_replace(crash_time_2, "\\s(AM|PM)", " \\1"),  # Remove the space before AM/PM
crash_time_2 = as.POSIXct(crash_time_2, format = "%I:%M %p"),  # Convert to POSIXct format
crash_date = as.Date(date))  # Convert 'date' to Date format
#mutate data3 to standard time
data3 <- data3 %>%
mutate(crash_date_time_standard = as.POSIXct(paste(crash_date, format(crash_time_2, "%H:%M:%S")), format = "%Y-%m-%d %H:%M:%S"))
#drop the excess columns we don't need anymore and make new polished data frame
data4 <- data3 %>%
select(-crash_date, -date, -crash_time_2,-crash_numb,-city_town_name)
#drop_na2()%>% #drop any row with NA
###BASIC ANALYSIS###
###Crash count vs time of day###
# Do some analysis on the timing of events
# Extract the hour from the crash_date_time_standard column
data4 <- data4 %>%
mutate(hour = hour(crash_date_time_standard))
# Calculate the crash count for each hour
crash_count <- data4 %>%
count(hour)
# Create a bar plot for crash count by hour
barplot(crash_count$n, names.arg = crash_count$hour, xlab = "Hour of Day", ylab = "Crash Count",
main = "Crash Count by Hour", col = "skyblue", border = "black", ylim = c(0, max(crash_count$n) + 10))
###Location in Boston map###
# Load required libraries
library(ggplot2)
library(ggmap)
#set the latitude and longitude boundaries for Boston area
boston_bounds <- c(left = -71.1912, bottom = 42.2279, right = -70.8085, top = 42.3974)
#get the map background using ggmap and specify the map type
boston_map <- get_stamenmap(boston_bounds, maptype = "toner-lite")
#plot the map of Boston
ggmap(boston_map) +
#add points representing crash locations
geom_point(data = data4, aes(x = lon, y = lat), color = "blue", alpha = 0.5) +
#adjust the transparency and color of the points
guides(alpha = FALSE) +
labs(title = "Crashes in Boston, MA") +
theme(plot.title = element_text(hjust = 0.5))
###MODEL ANALYSIS###
#---- logistic regression
data4 <- na.omit(data4)
# Loading caret library
library(caret)
# Splitting the data into train and test
index <- createDataPartition(data4$severity, p = .70, list = FALSE)
train <- data4[index, ]
test <- data4[-index, ]
# Training the model
logistic_model <- glm(severity ~ ., family = binomial(), train)
# Checking the model
summary(logistic_model)
pred_prob_lg <- predict(logistic_model, test, type = "response")
train$pred_class <- ifelse(logistic_model$fitted.values>=0.5, "Yes", "No")
# Generating the classification table
ctab_train <- table(train$severity, train$pred_class)
ctab_train
# Converting from probability to actual output
test$pred_class <- ifelse(pred_prob_lg >= 0.5, "Yes", "No")
# Generating the classification table
ctab_test <- table(test$severity, test$pred_class)
ctab_test
#Accuracy = (TP + TN)/(TN + FP + FN + TP)
# Accuracy in Training dataset
accuracy_train <- sum(diag(ctab_train))/sum(ctab_train)*100
accuracy_train
# Accuracy in Test dataset
accuracy_test <- sum(diag(ctab_test))/sum(ctab_test)*100
accuracy_test
Recall <- (ctab_train[2, 2]/sum(ctab_train[2, ]))*100
Recall
TNR <- (ctab_train[1, 1]/sum(ctab_train[1, ]))*100
TNR
Precision <- (ctab_train[2, 2]/sum(ctab_train[, 2]))*100
Precision
F_Score <- (2 * Precision * Recall / (Precision + Recall))/100
F_Score
#library(pROC)
#roc <- roc(train$Class, logistic_model$fitted.values)
#auc(roc)
#---- SVM-Radial
library(e1071)
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(data4), 0.7 * nrow(data4))  # 70% for training
train_data <- data4[train_indices, ]
test_data <- data4[-train_indices, ]
# Train the SVM model
svm_model <- svm(formula = severity ~ ., kernel = "radial", data = train_data)
# Make predictions on the test data
y_pred <- predict(svm_model, newdata = test_data)
y_pred <- ifelse(y_pred >= 0.5, "1", "0")
# Calculate accuracy
accuracy <- sum(y_pred == test_data$severity) / nrow(test_data)
print(paste("Accuracy:", accuracy))
# Calculate confusion matrix
confusion <- table(Actual = test_data$severity, Predicted = y_pred)
print("Confusion Matrix:")
print(confusion)
# Calculate precision, recall, and F1-score
#precision <- confusion[2, 2] / sum(confusion[, 2])
#recall <- confusion[2, 2] / sum(confusion[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
